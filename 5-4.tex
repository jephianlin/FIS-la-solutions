\section{Invariant Subspace and the Cayley-Hamilton Theorem}
\begin{enumerate}
\item \begin{enumerate}
\item No. The subspace $\{0\}$ must be a $T$-invariant subspace.
\item Yes. This is Theorem 5.21.
\item No. For example, let $T$ be the identity map from $\R$ to $\R$ and $v=(1)$ and $w=(2)$. Then we have $W=W'=\R$.
\item No. For example, let $T$ be the mapping from $\R^2$ to $\R^2$ defined by $T(x,y)=y$. Pick $v=(1,1)$. Thus the $T$-cyclic subspace generated by $v$ and $T(v)$ are $\R^2$ and the $x$-axis.
\item Yes. The \charpoly{} is the described polynomial.
\item Yes. We may prove it by induction or just use Exercise 5.1.21.
\item Yes. This is Theorem 5.25.
\end{enumerate}
\item \begin{enumerate}
\item Yes. For every $ax^2+bx+c\in W$, we have 
\[T(ax^2+bx+c)=2ax+b\in W.\]
\item No.  The space $V$ does not contain $W$ as a subspace.
\item Yes. For every $(t,t,t)\in W$, we have 
\[T(t,t,t)=(3t,3t,3t)\in W.\]
\item Yes. For every $at+b\in W$, we have 
\[T(at+b)=(\frac{a}{2}+b)t\in W.\]
\item No. For $\begin{pmatrix}1&0\\0&2\end{pmatrix}\in W$, we have
\[T(A)=\begin{pmatrix}0&2\\1&0\end{pmatrix}\notin W.\]
\end{enumerate}
\item \begin{enumerate}
\item We have $T(0)=0$ and $T(v)\in V$ any $v\in V$ and for arbitrary linear operator $T$ on $V$.
\item If $v\in N(T)$, we have $T(v)=0\in N(T)$. If $v\in R(T)$, we have $T(v)\in R(T)$ by the definition of $R(T)$.
\item If $v\in E_{\lambda}$, we have $T(v)=\lambda v$. Since $T(\lambda v)=\lambda T(v)=\lambda^2 v$, we know that $T(v)\in E_{\lambda}$.
\end{enumerate}
\item We have 
\[T^k(W)\subset T^{k-1}(T(W))\subset T^{k-1}(W)\subset \cdots \subset W.\]
For any $v\in W$, we know that $g(T)(v)$ is the sum of some elements in $W$. Since $W$ is a subspace, we know that $g(T)(v)$ is always an element is $W$.
\item Let $\{W_i\}_{i\in I}$ be the collection of $T$-invatirant subspaces and $W$ be the intersection of them. For every $v\in W$, we have $T(v)\in W_i$ for every $i\in I$, since $v$ is an element is each $W_i$. This means $T(v)$ is also an element in $W$.
\item Follow the prove of Theorem 5.22. And we know that the dimension is the maximum number $k$ such that 
\[\{z,T(z),T^2(z),\ldots ,T^{k-1}(z)\}\]
is independent and the set is a basis of the subspace.
\begin{enumerate}
\item Calculate that 
\[z=(1,0,0,0), T(z)=(1,0,1,1),\]
\[T^2(z)=(1,-1,2,2), T^3(z)=(0,-3,3,3).\]
So we know the dimension is $3$ and the set
\[\{z,T(z),T^2(z)\}\]
is a basis.
\item Calculate that
\[z=x^3,T(z)=6x,T^2(z)=0.\]
So we know that the dimension is $2$ and the set 
\[\{z,T(z)\}\]
is a basis.
\item Calculate that $T(z)=z$. So we know that the dimension is $1$ and $\{z\}$ is a basis.
\item Calculate that 
\[z=\begin{pmatrix}0&1\\1&0\end{pmatrix},T(z)=\begin{pmatrix}1&1\\2&2\end{pmatrix},\]
\[T^2(z)=\begin{pmatrix}3&3\\6&6\end{pmatrix}.\]
So we know that the dimension is $2$ and the set 
\[\{z,T(z)\}\]
is a basis.
\end{enumerate}
\item Let $W$ be a $T$-invariant subspace and $T_W$ be the restricted operator on $W$. We have that 
\[R(T_W)=T_W(W)=T(W)\subset W.\]
So at least it's a well-defined mapping. And we also have 
\[T_W(x)+T_W(y)=T(x)+T(y)=T(x+y)=T_W(x+y)\]
and 
\[T_W(cx)=T(cx)=cT(x)=cT_W(x).\]
So the restriction of $T$ on $W$ is also a lineaer operator.
\item If $v$ is an eigenvector of $T_W$ corresponding eigenvalue $\lambda$, this means that $T(v)=T_W(v)=\lambda v$. So the same is true for $T$.
\item See Example 5.4.6.
\begin{enumerate}
\item For the first method, we may calculate $T^3(z)=(3,-3,3,3)$ and represent it as a linear combination of the basis 
\[T^3(z)=0z-3T(z)+3T^2(z).\]
So the \charpoly{} is $-t^3+3t^2-3t$.
For the second method, denote $\beta$ to be the ordered basis 
\[\{z,T(z),T^2(z),T^3(z)\}.\]
And we may calculate the matrix representation 
\[[T_W]_{\beta}=\begin{pmatrix}0&0&0\\1&0&-3\\0&1&3\end{pmatrix}\]
and directly find the \charpoly{} of it to get the same result.
\item For the first method, we may calculate $T^3(z)=0$ and represent it as a linear combination of the basis 
\[T^2(z)=0z+0T(z).\]
So the \charpoly{} is $t^2$.
For the second method, denote $\beta$ to be the ordered basis 
\[\{z,T(z)\}.\]
And we may calculate the matrix representation 
\[[T_W]_{\beta}=\begin{pmatrix}0&0\\1&0\end{pmatrix}\]
and directly find the \charpoly{} of it to get the same result.
\item For the first method, we may calculate $T(z)=z$. So the \charpoly{} is $-t+1$.
For the second method, denote $\beta$ to be the ordered basis $\{z\}$.
And we may calculate the matrix representation 
\[[T_W]_{\beta}=\begin{pmatrix}1\end{pmatrix}\]
and directly find the \charpoly{} of it to get the same result.
\item For the first method, we may calculate $T^2(z)=3T(z)$. So the \charpoly{} is $t^2-3t$.
For the second method, denote $\beta$ to be the ordered basis 
\[\{z,T(z)\}.\]
And we may calculate the matrix representation 
\[[T_W]_{\beta}=\begin{pmatrix}0&0\\1&3\end{pmatrix}\]
and directly find the \charpoly{} of it to get the same result.
\end{enumerate}
\item Calculate the \charpoly{} is the problem of the first section and determine whether on polynomial is divided by the other is the problem in senior high.
\begin{enumerate}
\item The \charpoly{} is 
$${t}^{4}-4{t}^{3}+6{t}^{2}-3t.$$
\item The \charpoly{} is $t^4$.
\item The \charpoly{} is 
$${t}^{4}-2{t}^{3}+2t-1.$$
\item The \charpoly{} is 
$${t}^{4}-6{t}^{3}+9{t}^{2}.$$
\end{enumerate}
\item \begin{enumerate}
\item Let $w$ be an element in $W$. We may express $w$ to be 
\[w=\sum_{i=0}^k{a_iT^i(v)}.\]
And thus we have 
\[T(w)=\sum_{i=0}^k{a_iT^{i+1}(v)}\in W.\]
\item Let $U$ be a $T$-invariant subspace of $V$ containing $v$. Since it's $T$-invariant, we know that $T(v)$ is an element in $U$. Inductively, we know that $T^k(v)\in U$ for all nonnegative integer $k$. By Theorem 1.5 we know that $U$ must contain $W$.
\end{enumerate}
\item Because $W$ is a $T$-invariant subspace, we know that $T(v_i)\in W$ for $v_i\in \gamma$. This means the representation of $T(v_i)$ corresponding the basis $\beta$ only use the vectors in $\gamma$ for each $v_i\in \gamma$. So one corner of the matrix representation would be zero.
\item If $w$ is an element in $W$, it's a linear combination of 
\[\{v,T(v),T^2(v),\ldots .\]
So $w=g(T)(v)$ for some polynomial $g$. Conversely, if $w=g(T)(v)$ for some polynomial $g$, this means $w$ is a linear combination of the same set. Hence $w$ is an element in $W$.
\item It's because that the set 
\[v,T(v),T^2(v),\ldots ,T^{k-1}(v)\]
is a basis of $W$ by Theorem 5.22, where $k$ is the dimension of $W$.
\item The question in the Warning is because the definition of $f(A)$ is not $\det(A-AI)$. To prove the version for matrix, we may apply the theorem to the linear transformation $L_A$. So we know that the \charpoly{} of $L_A$ is the same as that of $A$, say $f(t)$. And we get the result $f(L_A)=T_0$ by the theorem. This means
\[f(A)v=f(L_A)(v)=0\]
for all $v$. So we know $f(A)=O$.
\item \begin{enumerate}
\item By Theorem 5.21 we know that the \charpoly{} of the restriction of $T$ to any $T$-invariant subspace is a factor of a polynomial who splits. So it splits, too.
\item Any nontrivial $T$-invariant subspace has dimension not equal to $0$. So the \charpoly{} of its restriction has degree greater than or equal to $1$. So it must contains at least one zero. This means the subspace at least contains one eigenvector.
\end{enumerate}
\item If we have the \charpoly{} to be  
\[f(t)=(-1)^nt^n+a_{n-1}t^{n-1}+\cdots +a_0,\]
then we have 
\[f(A)=(-1)^nA^n+a_{n-1}A^{n-1}+\cdots +a_0I=O\]
by Cayley-Hamilton Theorem. This means that $A^n$ is a linear combination of $I,A,A^2,\ldots ,A^{n-1}$. By multiplying both sides by $A$, we know that $A^{n+1}$ is a linear combination of $A,A^2,\ldots ,A^{n}$. Since $A^n$ can be represented as a linear combination of previous terms, we know that $A^{n+1}$ could also be a linear combination of $I,A,A^2,\ldots ,A^{n-1}$. Inductively, we know that 
\[\sp\{I,A,A^2,\ldots \}=\sp\{I,A,A^2,\ldots ,A^{n-1}\}\]
and so the dimension could not be greater than $n$.
\item \begin{enumerate}
\item See Exercise 5.1.20.
\item For simplicity, denote the right hand side to be $B$. Directly calculate that 
\[AB=-\frac{1}{a_0}[(-1)^nA^n+a_{n-1}A^{n-1}+\cdots +a_1A]\]
\[-\frac{1}{a_0}(-a_0I)=I.\]
\item Calculate that the \charpoly{} of $A$ is 
$$-{t}^{3}+2{t}^{2}+t-2.$$
So by the previous result, we know that 
\[A^{-1}=\frac{1}{2}[-{A}^{2}+2{A}+I\]
\[=\begin{pmatrix}1 & -1 & -2\cr 0 & \frac{1}{2} & \frac{3}{2}\cr 0 & 0 & -1\end{pmatrix}.\]
\end{enumerate}
\item As Hint, we induct on $k$. For $k=1$, the matrix is $\begin{pmatrix}-a_0\end{pmatrix}$ and the \charpoly{} is $-(a_0+t)$. If the hypothesis holds for the case $k=m-1$, we may the expand the matrix by the first row and calculate the \charpoly{} to be 
\[\det(A-tI)=\det\begin{pmatrix}-t&0&\cdots &-a_0\\1&-t&\cdots&-a_1\\\vdots &\vdots&\ddots &\vdots \\0&0&\cdots &-a_{m-1}\end{pmatrix}\]
\[=-t\det\begin{pmatrix}-t&0&\cdots &-a_0\\1&-t&\cdots&-a_1\\\vdots &\vdots&\ddots &\vdots \\0&0&\cdots &-a_{m-2}\end{pmatrix}+(-1)^{m+1}(-a_0)\det\begin{pmatrix}1&-t&\cdots &0\\0&1&\cdots &-a_1\\\vdots &\vdots &\ddots&\vdots \\0&0&\cdots &1\end{pmatrix}\]
\[=-t[(-1)^{m-1}(a_1+a_2t+\cdots +a_{m-1}t^{m-2}+t^{m-1})]+(-1)^ma_0\]
\[=(-1)^m(a_0+a_1t+\cdots +a_{m-1}t^{m-1}+t^m).\]
\item If $U=g(T)$, we know that $UT=TU$ since 
\[T(T^k)=(T^k)T=T^{k+1}\]
and $T$ is linear. For the converse, we may suppose that $V$ is generated by $v$. Then the set 
\[\beta=\{v,T(v),T^2(v),\ldots ,T^k(v)\}\]
is a basis. So the vector $U(v)$ could be written as a linear combination of $\beta$. This means $U(v)=g(T)(v)$ for some polynomial $g$. Now if $UT=TU$, we want to show $U=g(T)$ by showing $U(w)=g(T)(w)$ for all $w\in \beta$. Observe that 
\[U(T^m(v))=T^m(U(v))=T^mg(T)(v)=g(T)(T^m(v))\]
for all nonnegative integer $m$. So we get the desired result.
\item If we have some vector $v\in V$ such that $v$ and $T(v)$ are not parallel, we know the $T$-cyclic subspace generated by $v$ has dimension $2$. This means $V$ is a $T$-cyclic subspace of itself generated by $v$. Otherwise for every $v\in V$, we may find some scalar $\lambda_v$ such that $T(v)=\lambda_vv$. Now, if $\lambda_v$ is a same value $c$ for every nonzero vector $v$, then we have $T=cI$. If not, we may find $\lambda_v\neq\lambda_u$ for some nonzero vector $v$ and $u$. This means $v$ and $u$ lies in different eigenspace and so the set $\{u,v\}$ is independent. Thus they forms a basis. Now let $w=v+u$. We have both
\[T(w)=\lambda_ww=\lambda_wv+\lambda_wu\]
and 
\[T(w)=\lambda_vv+\lambda_uu.\]
By the uniqueness of representation of linear combination, we must have 
\[\lambda_v=\lambda_w=\lambda_u,\]
a contradiction. So in this case we must have $T=cI$.
\item If $T\neq cI$, then $V$ must be a $T$-cyclic subspace of itself by Exercise 5.4.21. So by Exercise 5.4.20, we get the desired result.
\item As Hint, we prove it by induction on $k$. For $k=1$, it's a natural statement that if $v_1$ is in $W$ then $v_1$ is in $W$. If the statement is true for $k=m-1$, consider the case for $k=m$. If we have $u=v_1+v_2+\cdots +v_m$ is in $W$, a $T$-invariant subspace, then we have 
\[T(u)=\lambda_1v_1+\lambda_2v_2+\cdots +\lambda_mv_m\in W,\]
where $\lambda_i$ is the distinct eigenvalues.
But we also have $\lambda_mu$ is in $W$. So the vector
\[T(u)-\lambda_mu=(\lambda_1-\lambda_m)v_1+(\lambda_2-\lambda_m)v_2+\cdots +(\lambda_{m-1}-\lambda_m)v_{m-1}\]
is al so in $W$. Since those eigenvalues are distinct, we have $\lambda_i-\lambda_m$ is not zero for all $i\neq m$. So we may apply the hypothesis to 
\[(\lambda_1-\lambda_m)v_1,(\lambda_2-\lambda_m)v_2+\cdots +(\lambda_{m-1}-\lambda_m)v_{m-1}\]
and get the result that $(\lambda_i-\lambda_m)v_i$ is in $W$ and so $v_i$ is in $W$ for all $i\neq m$. Finally, we still have 
\[v_m=u-v_1-v_2-\cdots -v_{m-1}\in W.\]
\item Let $T$ be a operator on $V$ and $W$ be a nontrivial $T$-invariant subspace. Also let $E_{\lambda}$ be the eigenspace of $V$ corresponding to the eigenvalue $\lambda$. We set $W_{\lambda}=E_{\lambda}\cap W$ to be the eigenspace of $T_W$ corresponding to the eigenvalue $\lambda$. We may find a basis $\beta_{\lambda}$ for $W_{\lambda}$ and try to show that $\beta=\cup_{\lambda}\beta_{\lambda}$ is a basis for $W$. The set $\beta $ is linearly independent by Theorem 5.8. Since $T$ is diagonalizable, every vector could be written as a linear combination of eigenvectors corresponding to distinct eigenvalues, so are those vectors in $W$. But by the previous exercise we know that those eigenvectors used to give a linear combination to elements in $W$ must also in $W$. This means every elements in $W$ is a linear combination of $\beta$. So $\beta $ is a basis for $W$ consisting of eigenvectors.
\item \begin{enumerate}
\item Let $E_{\lambda}$ be a eigenspace of $T$ corresponding to the eigenvalue $\lambda$. For every $v\in E_{\lambda}$ we have 
\[TU(v)=UT(v)=\lambda U(v).\]
This means that $E_{\lambda}$ is an $U$-invariant subspace. Applying the previous exercise to each $E_{\lambda}$, we may find a basis $\beta_{\lambda}$ for $E_{\lambda}$ such that $[U_{E_{\lambda}}]_{\beta_{\lambda}}$ is diagonal. Take $\beta $ to be the union of each $\beta_{\lambda}$ and then both $[T]_{\beta}$ and $[U]_{\beta}$ are diagonal simultaneously.
\item Let $A$ and $B$ are two $n\times n$ matrices. If $AB=BA$, then $A$ and $B$ are simultaneously diagonalizable. To prove this we may apply the version of linear transformation to $L_A$ and $L_B$.
\end{enumerate}
\item Let $\{v_1,v_2,\ldots ,v_n$ be those eigenvectors corresponding to $n$ distinct eigenvalues. Pick 
\[v=v_1+v_2+\cdots +v_n\]
and say $W$ to be the $T$-cyclic subspace generated by $v$. By Exercise 5.4.23 we know that $W$ must contains all $v_i$'s. But this means the dimension of $W$ is $n$ and the set 
\[\{v,T(v),\ldots ,T^{n-1}(v)\}\]
is a basis by Theorem 5.22.
\item \begin{enumerate}
\item If $v+W=v'+W$, we know that $v-v'$ is an element in $W$ by Exercise 1.3.31. Since $W$ is $T$-invariant, we have 
\[T(v)-T(v')=T(v-v')\]
is in $W$. So we have 
\[T(v)+W=T(v')+W\]
and this means
\[\overline{T}(v+W)=\overline{T}(v'+W).\]
\item Just check that 
\[\overline{T}((v+v')+W)=T(v+v')+W\]
\[=(T(v)+W)+(T(v')+W)=\overline{T}(v+W)+\overline{T}(v'+W)\]
and 
\[\overline{T}(cv+W)=T(cv)+W=c(T(v)+W)=c\overline{T}(v+W).\]
\item For each $v\in V$ we might see
\[\eta T(v)=T(v)+W=\overline{T}(v+W)=\overline{T}\eta(v).\]
\end{enumerate}
\item We use the notation given in Hint. Since $W$ is $T$-invariant, we know the matrix representation of $T$ is 
\[[T]_{\beta}=\begin{pmatrix}B_1&B_2\\O&B_3\end{pmatrix}.\]
As the proof of Theorem 5.21, we know that $f(t)=\det([T]_{\beta}-tI)$ and $g(t)=\det(B_1-tI)$. It's enough to show $h(t)=\det(B_3-tI)$ by showing $B_3$ is a matrix representation of $\overline{T}$. Let 
\[\alpha=\{v_k+W,v_{k+1}+W,\ldots ,v_n+W\}\]
be a basis for $V/W$ by Exercise 1.6.35. Then for each $j=k,k+1,\ldots ,n$, we have 
\[\overline{T}(v_j)=T(v_j)+W\]
\[=[\sum_{l=1}^k{(B_2)_{lj}v_l}+\sum_{i=k+1}^n{(B_3)_{ij}v_i}]+W\]
\[=\sum_{i=k+1}^n{(B_3)_{ij}v_i}+W=\sum_{i=k+1}^n{(B_3)_{ij}(v_i+W)}.\]
So we have $B_3=[\overline{T}]_{\alpha}$ and 
\[f(t)=\det([T]_{\beta}-tI)=\det(B_1-tI)\det(B_3-tI)=g(t)h(t).\]
\item We use the notation given in Hint of the previous exercise again. By Exercise 5.4.24 we may find a basis $\gamma$ for $W$ such that $[T_W]_{\gamma}$ is diagonal. For each eigenvalue $\lambda$, we may pick the corresponding eigenvectors in $\gamma$ and extend it to be a basis for the corresponding eigenspace. By collecting all these basis, we may form a basis $\beta$ for $V$ who is union of these basis. So we know that $[T]_{\beta}$ is diagonal. Hence the matrix $B_3$ is also diagonal. Since we've proven that $B_3=[\overline{T}]_{\alpha}$, we know that $\overline{T}$ is diagonalizable.
\item We also use the notation given in Hint of the previous exercise. By Exercise 5.4.24 we may find a basis 
\[\gamma=\{v_1,v_2,\ldots ,v_k\}\]
for $W$ such that $[T_W]_{\gamma}$ is diagonal and find a basis 
\[\alpha=\{v_{k+1}+W,v_{k+2}+W,\ldots ,v_n+W\}\]
for $V/W$ such that $[\overline{T}]_{\alpha}$ is diagonal. For each $v+W\in V/W$, we know that $v+W$ is an eigenvector in $V/W$. So we may assume that 
\[T(v)+W=\overline{T}(v+W)=\lambda v+W\]
for some scalar $\lambda$.
So this means that $T(v)-\lambda v$ is an element in $W$. Write 
\[T(v)=\lambda v+a_1v_1+a_2v_2+\cdots +a_kv_k,\]
where $v_i$'s are those elements in $\gamma$ corresponding to eigenvalues $\lambda_i$'s.
Pick $c_i$ to be $\frac{a_i}{\lambda-\lambda_i}$ and set 
\[v'=v+c_1v_1+c_2v_2+\cdots +c_kv_k.\]
Those $c_i$'s are well-defined because $T_W$ and $\overline{T}$ has no common eigenvalue.
Thus we have 
\[T(v')=T(v)+c_1T(v_1)+c_2T(v_2)+\cdots +c_kT(v_k)\]
\[=\lambda v+(a_1+c_1\lambda_1)v_1+(a_2+c_2\lambda_2)v_2+\cdots +(a_k+c_k\lambda_k)v_k)\]
\[=\lambda v'\]
and $v+W=v'+W$.
By doing this process, we may assume that $v_j$ is an eigenvector in $V$ for each $v_j+W\in \alpha$. Finally we pick 
\[\beta=\{v_1,v_2,\ldots ,v_n\}\]
and show that it's a basis for $V$. We have that $\gamma $ is independent and 
\[\delta=\{v_{k+1},v_{k+2},\ldots ,v_n\}\]
is also independent since $\eta(\delta)=\alpha$ is independent, where $\eta$ is the quotient mapping defined in Exercise 5.4.27. Finally we know that if 
\[a_{k+1}v_{k+1}+a_{k+2}v_{k+2}+\cdots a_nv_n\in W,\]
then 
\[(a_{k+1}v_{k+1}+a_{k+2}v_{k+2}+\cdots a_nv_n)+W=0+W\]
and so 
\[a_{k+1}=a_{k+2}=\cdots =a_n=0.\]
This means that $\sp(\delta)\cap W=\{0\}$. So $V$ is a directed sum of $W$ and $\sp(\delta)$ and $\beta$ is a basis by Exercise 1.6.33. And thus we find a basis consisting of eigenvectors.
\item \begin{enumerate}
\item Compute 
\[T(e_1)=\begin{pmatrix}1\\2\\1\end{pmatrix},T^2(e_1)=\begin{pmatrix}0\\12\\6\end{pmatrix}\]
and 
\[T^2(e_1)=-6e_1+6T(e_1).\]
This means that the \charpoly{} of $T_W$ is $t^2-6t+6$.
\item We know that 
\[\dim(\R^3/W)=3-\dim(W)=1\]
by Exercise 1.6.35. So every nonzero element in $\R^3/W$ is a basis. Since $e_2$ is not in $W$, we have $e_2+W$ is a basis for $\R^3/W$. Now let $\beta=\{e_2+W\}$. We may compute 
\[\overline{T}(e_2+W)=\begin{pmatrix}1\\3\\2\end{pmatrix}+W=-e_2+W\]
and $[\overline{T}]_{\beta}=\begin{pmatrix}1\end{pmatrix}$. So the \charpoly{} of $\overline{T}$ is $-t-1$.
\item Use the result in Exercise 5.4.28, we know the \charpoly{} of $T$ is 
\[-(t+1)(t^2-6t+t).\]
\end{enumerate}
\item As Hint, we induct on the dimension $n$ of the space. For $n=1$, every matrix representation is an uppertriangular matrix. Suppose the hypothesis is true for $n=k-1$. Now we consider the case $n=k$. Since the \charpoly{} splits, $T$ must has an eigenvector $v$ corresponding to eigenvalue $\lambda$. Let $W=\sp(\{v\})$. By Exercise 1.6.35 we know that the dimension of $V/W$ is equal to $k-1$. So we apply the induction hypothesis to $\overline{T}$ defined in Exercise 5.4.27. This means we may find a basis 
\[\alpha=\{u_2+W,u_3+W,\ldots ,u_k+W\}\]
for $V/W$ such that $[\overline{T}]_{\alpha}$ is an upper triangular matrix. Following the argument in Exercise 5.4.30, we know that 
\[\beta=\{v,u_2,u_3,\ldots,u_k\}\]
is a basis for $V$. And we also know the matrix representation is 
\[[T]_{\beta}=\begin{pmatrix}\lambda &*\\O&[\overline{T}]_{\alpha}\end{pmatrix},\]
which is an upper triangular matrix since $[\overline{T}]_{\alpha}$ is upper triangular.
\item Let 
\[w=w_1+w_2+\cdots +w_k\]
for some $w_i\in W_i$. Thus we have $T(w_i)\in W_i$ since $W_i$ is $T$-invariant. So we also have 
\[T(w)=T(w_1)+T(w_2)+\cdots +T(w_k)\in W_1+W_2+\cdots W_k.\]
\item In the next exercise we prove this exercise and the next exercise together.
\item Let $v$ is an element in $\beta_1$. We know that $T(v)$ is a linear combination of $\beta$. Since $T(v)$ is in $W_1$, we know that the combination only use elements in $\beta_1$. The same argument could be applied to each element in $\beta_i$ for each $i$. So we know the matrix representation of $T$ corresponding to the chosen basis $\beta $ is 
\[B_1\oplus B_2\oplus \cdots \oplus B_k.\]
\item First observe that a one-dimensional $T$-invariant subspace is equivalent to a subspace spanned by an eigenvector. Second, $V$ is the direct sum of some one-dimensional subspaces if and only if the set obtained by choosing one nonzero vector from each subspace forms a basis. Combining these two observation we get the result.
\item Use Theorem 5.25 and its notation. We have 
\[\det(T)=\det(B)=\det(B_1)\det(B_2)\cdots \det(B_k)\]
\[=\det(T_{W_1})\det(T_{W_2})\cdots \det(T_{W_k}).\]
\item By Exercise 5.4.24, we already get the necessity. For the sufficiency, we may pick bases $\beta_i$ for $W_i$ such that $[T_{W_i}]$ is diagonal. Combining these bases to be a basis $\beta=\cup_{i=1}^k{\beta_i}$ for $V$. By using Theorem 5.25, we know that $T$ is diagonalizable.
\item By Exercise 5.2.18, we already get the necessity. For the sufficiency, we use induction on the dimension $n$ of the spaces. If $n=1$, every operator on it is diagonalizable by the standard basis. By supposing the statement is true for $n\leq k-1$, consider the case $n=k$. First, if all the operator in $\mathcal{C}$ has only one eigenvalue, then we may pick any basis $\beta$ and know that $[T]_{\beta}$ is diagonal for all $T\in \mathcal{C}$. Otherwise, there must be one operator $T$ possessing two or more eigenvalues, $\lambda_1,\lambda_2,\ldots ,\lambda_t$. Let $W_i$ be the eigenspace of $T$ corresponding to the eigenvalue $\lambda_i$. We know that $V$ is the direct sum of all $W_i$'s. By the same reason in the proof of Exercise 5.4.25, we know that $W_i$ is a $U$-invariant subspace for all $U\in \mathcal{C}$. Thus we may apply the induction hypothesis on $W_i$ and 
\[\mathcal{C}_i:=\{U_{W_i}:U\in \mathcal{C}\}.\]
Thus we get a basis $\beta_i$ for $W_i$ such that $[U_{W_i}]_{\beta_i}$ is diagonal. Let $\beta$ be the union of each $\beta_i$. By applying Theorem 5.25, we get the desired result.
\item Observe that 
\[A-tI=(B_1-tI)\oplus (B_2-tI)\oplus \cdots \oplus (B_k-tI).\]
So we have 
\[\det(A-tI)=\prod_{i=1}^k{\det(B_i-tI)}.\]
\item Let $v_i$ be the $i$-th row vector of $A$. We have $\{v_1,v_2\}$ is linearly independent and $v_i=(i-1)(v_2-v_1)+v_1$. So the rank of $A$ is $2$. This means that $t^{n-2}$ is a factor of the \charpoly{} of $A$. Finally, set 
\[\beta=\{(1,1,\ldots ,1),(1,2,\ldots ,n)\}\]
and check that $W=\sp(\beta)$
is a $L_A$-invariant subspace by computing  
\[A\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix}=(\frac{n(n+1)}{2}-n^2)\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix}+n^2\begin{pmatrix}1\\2\\\vdots\\n\end{pmatrix}\]
and 
\[A\begin{pmatrix}1\\2\\\vdots\\n\end{pmatrix}=(\frac{n(n+1)(2n+1)}{6}-\frac{n^2(n+1)}{2})\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix}+\frac{n^2(n+1)}{2}\begin{pmatrix}1\\2\\\vdots\\n\end{pmatrix}.\]
So we know the \charpoly{} is 
\[(-1)^nt^{n-2}\det\begin{pmatrix}\frac{n(n+1)}{2}-{n}^{2}-t& \frac{n(n+1)(2n+1)}{6}-\frac{{n}^{2}(n+1)}{2}\cr {n}^{2} & \frac{{n}^{2}(n+1)}{2}-t\end{pmatrix}\]
\[=(-1)^nt^{n-2}\frac{12{t}^{2}+(-6{n}^{3}-6n)t-{n}^{5}+{n}^{3}}{12}.\]
But this is the formula for $n\geq 2$. It's natural that when $n=1$ the \charpoly{} is $1-t$. Ur...I admit that I computed this strange answer by wxMaxima.
\item Observe that the nullity of $A$ is $n-1$ and $\det(A-nI)=0$ because the sum of all column vectors of $A-nI$ is zero. So we know the \charpoly{} of it is 
\[(-1)^nt^{n-1}(t-n).\]
\end{enumerate}
