\section{Diagonalizability}
\begin{enumerate}
\item \begin{enumerate}
\item No. The matrix 
\[\begin{pmatrix}1&0&0\\0&0&0\\0&0&0\end{pmatrix}\]
has only two distinct \egva s but it's diagonalizable.
\item No. The vectors $\begin{pmatrix}1\\0\end{pmatrix}$ and $\begin{pmatrix}2\\0\end{pmatrix}$ are both the \egve s of the matrix $\begin{pmatrix}1&0\\0&0\end{pmatrix}$ corresponding the same \egva{} $1$.
\item No. The zero vector is not.
\item Yes. If $x\in E_{\lambda_1}\cap E_{\lambda_2}$, we have $\lambda_1x=Ax=\lambda_2x$. It's possible only when $x=0$.
\item Yes. By the hypothesis, we know $A$ is diagonalizable. Say $A=P^{-1}DP$ for some invertible matrix $P$ and some diagonal matrix $D$. Thus we know that 
\[Q^{-1}AQ=(PQ)^{-1}D(PQ).\]
\item No. It need one more condition that the \charpoly{} spilts. For example, the matrix $\begin{pmatrix}2&1\\3&2\end{pmatrix}$ has no real \egva .
\item Yes. Since it's a diagonalizable operator on nonzero vector space, it's \charpoly{} spilts with degree greater than or equal to $1$. So it has at least one zero.
\item Yes. Because we have 
\[W_i\cup \sum_{i\neq k}{W_k}=\{0\}\]
and 
\[\sum_{i\neq k}{W_k}=\{0\}\supset W_j\]
for all $j\neq i$, we get the desired answer.
\item No. For example, take $W_1=\sp\{(1,0)\}$, $W_2=\sp\{(0,1)\}$, and $W_3=\sp\{(1,1)\}$.
\end{enumerate}
\item For these question, see the direction of the subsection ``Test for Diagonalization''.
\begin{enumerate}
\item It's not diagonalizable since $\dim(E_1)$ is $1$ but not $2$.
\item It's diagonalizable with $D=\begin{pmatrix}-2&0\\0&4\end{pmatrix}$ and $Q=\begin{pmatrix}1 & 1\cr -1 & 1\end{pmatrix}$.
\item It's diagonalizable with $D=\begin{pmatrix}-2 & 0\cr 0 & 5\end{pmatrix}$ and $Q=\begin{pmatrix}4 & 1\cr -3 & 1\end{pmatrix}$.
\item It's diagonalizable with $D=\begin{pmatrix}3 & 0 & 0\cr 0 & 3 & 0\cr 0 & 0 & -1\end{pmatrix}$ and $Q=\begin{pmatrix}1 & 0 & 2\cr 1 & 0 & 4\cr 0 & 1 & 3\end{pmatrix}$.
\item It's not diagonalizable since its \charpoly{} does not split.
\item It's not diagonalizable since $\dim(E_1)$ is $1$ but not $2$.
\item It's diagonalizable with $D=\begin{pmatrix}4 & 0 & 0\cr 0 & 2 & 0\cr 0 & 0 & 2\end{pmatrix}$ and $Q=\begin{pmatrix}1 & 1 & 0\cr 2 & 0 & 1\cr -1 & -1 & -1\end{pmatrix}$.
\end{enumerate}
\item For these question, we may choose arbitrary matrix representation, usually use the standard basis, and do the same as what we did in the previous exercises. So here we'll have $[T]_{\beta}=D$ and the set of column vectors of $Q$ is the ordered basis $\beta$.
\begin{enumerate}
\item It's not diagonalizable since $\dim(E_0)$ is $1$ but not $4$.
\item It's diagonalizable with $D=\begin{pmatrix}-1 & 0 & 0\cr 0 & 1 & 0\cr 0 & 0 & 1\end{pmatrix}$ and $Q=\begin{pmatrix}1 & 1 & 0\cr 0 & 0 & 1\cr -1 & 1 & 0\end{pmatrix}$.
\item It's not diagonalizable since its \charpoly{} does not split.
\item It's diagonalizable with $D=\begin{pmatrix}1 & 0 & 0\cr 0 & 2 & 0\cr 0 & 0 & 0\end{pmatrix}$ and $Q=\begin{pmatrix}-1 & 0 & 0\cr 1 & 1 & -1\cr 1 & 1 & 1\end{pmatrix}$.  Note that we used the basis $\{1, x, x^2\}$.  
\item It's diagonalizable with $D=\begin{pmatrix}1-i & 0\cr 0 & i+1\end{pmatrix}$ and $Q=\begin{pmatrix}1 & 1\cr -1 & 1\end{pmatrix}$.
\item It's diagonalizable with $D=\begin{pmatrix}-1 & 0 & 0 & 0\cr 0 & 1 & 0 & 0\cr 0 & 0 & 1 & 0\cr 0 & 0 & 0 & 1\end{pmatrix}$ and $Q=\begin{pmatrix}0 & 1 & 0 & 0\cr 1 & 0 & 1 & 0\cr -1 & 0 & 1 & 0\cr 0 & 0 & 0 & 1\end{pmatrix}$.
\end{enumerate}
\item It's not funny again. Replace the character $T$ by the character $A$ in that prove.
\item It's not funny again and again. Replace the character $T$ by the character $A$ in that prove.
\item \begin{enumerate}
\item An operator $T$ is diagonalizable ensure that its \charpoly{} splits by Theorem 5.6. And in this situation Theorem 5.9(a) ensure that the multiplicity of each \egva{} meets the dimension of the corresponding eigenspace. Conversly, if the \charpoly{} splits and the multiplicity meets the dimension, then the operator will be diagonalizable by Theorem 5.9(a).
\item Replace $T$ by $A$ again.
\end{enumerate}
\item Diagonalize the matrix $A$ by $Q^{-1}AQ=D$ with $D=\begin{pmatrix}5 & 0\cr 0 & -1\end{pmatrix}$ and $Q=\begin{pmatrix}1 & 2\cr 1 & -1\end{pmatrix}$. So we know that 
\[A^n=QD^nQ^{-1}=Q\begin{pmatrix}5^n & 0\cr 0 & (-1)^n\end{pmatrix}Q^{-1}.\]
\item We know that $\dim(E_{\lambda_2})\geq 1$. So pick a nonzero vector $v\in E_{\lambda_2}$. Also pick a basis $\beta $ for $E_{\lambda_2}$. Then $\alpha=\beta\cup \{v\}$ forms a basis consisting of eigenvectors. It's a basis because the cardinality is $n$ and the help of Theorem 5.8.
\item \begin{enumerate}
\item Because the \charpoly{} of $T$ is independent of the choice of $\beta $, we know that the \charpoly{} 
\[f(t)=\det([T]_{\beta}-tI)=\prod_{i=1}^n{(([T]_{\beta})_{ii}-t)}\]
splits, where the second equality holds since it's a upper triangular matrix.
\item The \charpoly{} of a matrix is also the same for all matrices which is similar to the original matrix.
\end{enumerate}
\item This is because the equality in Exercise 5.2.9(a). That is, if $[T]_{\beta }$ is an upper triangular matrix, it's diagonal entries are the set of zeroes of the \charpoly{}.
\item Recall that similar matrices have the same trace and the same determinant; see Exercises~5.1.7(a) and 5.1.16(a).  Suppose $U$ is the uper triangular matrix that $A$ is similar to.  According to Exercise 5.2.10, the diagonal entries of $U$ are $\lambda_1$, $\lambda_2$, $\ldots$, $\lambda_k$ and that each $\lambda_i$ occurse $m_i$ times.  Thus, the value of $\tr(A) = \tr(U)$ is the sum of diagonal entries of $U$, which answers part (a).  On the other hand, since $U$ is an upper triangular matrix, its determinant is the product of its diagonal entries, which gives (b).
%% \begin{enumerate}
%% \item Since \egva s are the zeroes of the \charpoly{}, we may write the \charpoly{} $f(t)$ as 
%% \[(\lambda_1-t)^{m_1}(\lambda_2-t)^{m_2}\cdots (\lambda_k-t)^{m_k}.\]
%% Calculate the coefficient of $t^{n-1}$ and use the fact in Exercise 5.1.21(b). Thus we could get the desired conclusion.
%% \item Use the equality in the previous exercise and calculate the coefficient of the constant term. Compare it to the Exercise 5.1.20.
%% \end{enumerate}
\item \begin{enumerate}
\item Let $E_{\lambda}$ be the eigenspace of $T$ corresponding to $\lambda$ and $E_{\lambda^{-1}}$ be the eigenspace of $T^{-1}$ corresponding to $\lambda^{-1}$. We want to prove the two spaces are the same. If $v\in E_{\lambda}$, we have $T(v)=\lambda v$ and so $v=\lambda T^{-1}(v)$. This means $T^{-1}(v)=\lambda^{-1}v$ and $v\in E_{\lambda^{-1}}$. Conversely, if $v\in E_{\lambda^{-1}}$, we have $T^{-1}(v)=\lambda^{-1}v$ and so $v=\lambda^{-1} T(v)$. This means $T(v)=\lambda v$ and $v\in E_{\lambda}$.
\item By the result of the previous exercise, if $T$ is diagonalizable and invertible, the basis consisting of \egve s of $T$ will also be the basis consisting of \egve s of $T^{-1}$.
\end{enumerate}
\item \begin{enumerate}
\item For matrix $A=\begin{pmatrix}1&0\\1&0\end{pmatrix}$, corresponding to the same eigenvalue $0$ we have $E_0=\sp\{(0,1)\}$ is the eigenspace for $A$ while $E_0=\sp\{(1,-1)\}$ is the eigenspace for $A^t$.
\item Observe that 
\[\dim(E_\lambda)=\nul(A-\lambda I)=\nul((A-\lambda I)^t)\]
\[=\nul(A^t-\lambda I)=\dim(E_\lambda').\]
\item If $A$ is diagonalizable, then its \charpoly{} splits and the multiplicity meets the dimension of the corresponding eigenspace. Since $A$ and $A^t$ has the same \charpoly{}, the \charpoly{} of $A^t$ also splits. And by the precious exercise we know that the multiplicity meets the dimension of the corresponding eigenspace in the case of $A^t$.
\end{enumerate}
\item \begin{enumerate}
\item Let $v=(x,y)$ and $v'=(x',y')$ and $A=\begin{pmatrix}1&1\\3&-1\end{pmatrix}$. We may write the system of equation as $Av=v'$. We may also diagonalize the matrix $A$ by $Q^{-1}AQ=D$ with $D=\begin{pmatrix}-2 & 0\cr 0 & 2\end{pmatrix}$ and $Q=\begin{pmatrix}1 & 1\cr -3 & 1\end{pmatrix}$. This means $D(Q^{-1}v)=(Q^{-1}v)'$. So we know that 
\[Q^{-1}v=\begin{pmatrix}c_1e^{-2t}\\c_2e^{2t}\end{pmatrix}\]
and
\[v=Q\begin{pmatrix}c_1e^{-2t}\\c_2e^{2t}\end{pmatrix},\]
where $c_i$ is some scalar for all $i$.
\item Calculate $D=\begin{pmatrix}3 & 0\cr 0 & -2\end{pmatrix}$ and $Q=\begin{pmatrix}2 & 1\cr -1 & -1\end{pmatrix}$. So we have 
\[v=Q\begin{pmatrix}c_1e^{3t}\\c_2e^{-2t}\end{pmatrix},\]
where $c_i$ is some scalar for all $i$.
\item Calculate $D=\begin{pmatrix}1 & 0 & 0\cr 0 & 1 & 0\cr 0 & 0 & 2\end{pmatrix}$ and $Q=\begin{pmatrix}1 & 0 & 1\cr 0 & 1 & 1\cr 0 & 0 & 1\end{pmatrix}$. So we have 
\[v=Q\begin{pmatrix}c_1e^{t}\\c_2e^{t}\\c_3e^{2t}\end{pmatrix},\]
where $c_i$ is some scalar for all $i$.
\end{enumerate}
\item For convenience, let the spectrum of $A$ be $\{\mu_1,\ldots, \mu_n\}$, where some of the eigenvalues might be the same, and $\{u_1, \ldots, u_n\}$ the corresponding eigenbasis.  Let $Q$ be the matrix whose columns are $u_1,\ldots, u_n$ and $D$ the diagonal matrix whose diagonal entries are $\mu_1, \ldots, \mu_n$.  Thus, $D = Q^{-1}AQ$ and the system of differential equations $\dot{x} = A x$ is equivalent to $\dot{y} = D y$, where $x = Qy$.  Since the general solution of $\dot{y} = D y$ is $y = (c_1e^{\mu_it}, \ldots, c_ne^{\mu_nt})$ for some constants $c_1, \ldots, c_n$.  We know $x = Qy$ and can be written as  
\[
    x(t) = e^{\mu_1t}c_1u_1 + e^{\mu_2t}c_2u_2 + \cdots + e^{\mu_nt}c_nu_n.
\]
Thus, we may merge the terms with the same eigenvalues and write
\[
    x(t) = \sum_{i=1}^k e^{\lambda_it}\left(\sum_{\mu_j = \lambda_i} c_ju_j\right).
\]
Since 
\[
    E_{\lambda_i} = \{\sum_{\mu_j = \lambda_i} c_ju_j : c_j\in\mathbb{R} \},
\]
the form we get for the general solution is equivalent to the one in the problem.  

To see that the general solution is an $n$-dimensional real vector space, it is enough to check the set $\{e^{\mu_1t}u_1, \ldots, e^{\mu_nt}u_n\}$ is linearly independent.  Suppose for some $c_1, \ldots, c_n\in\mathbb{R}$ we have 
\[
    c_1e^{\mu_1t}u_1 + c_2e^{\mu_2t}u_2 + \cdots + c_ne^{\mu_nt}u_n = 0.
\]
By evaluating the equation at $t = 0$, we konw  
\[
    c_1u_1 + c_2u_2 + \cdots + c_nu_n = 0.
\]
Since $\{u_1, \ldots, u_n\}$ is an eigenbasis and is linearly independent, we have $c_1 = \cdots = c_n = 0$ and the set $\{e^{\mu_1t}u_1, \ldots, e^{\mu_nt}u_n\}$ is linearly independent.
\item Directly calculate that 
\[(CY)'_{ij}=(\sum_{k=1}^n{C_{ik}Y_{kj}})'\]
\[=\sum_{k=1}^n{C_{ik}Y_{kj}'}=CY'.\]
\item \begin{enumerate}
\item We may pick one basis $\alpha $ such that both $[T]_{\alpha}$ and $[U]_{\alpha}$ are diagonal. Let $Q=[I]_{\alpha}^{\beta}$. And we will find out that 
\[[T]_{\alpha}=Q^{-1}[T]_{\beta}Q\]
and 
\[[U]_{\alpha}=Q^{-1}[U]_{\beta}Q.\]
\item Let $Q$ be the invertible matrix who makes $A$ and $B$ simultaneously diagonalizable. Say $\beta $ be the basis consisting of the column vectors of $Q$. And let $\alpha $ be the standard basis. Now we know that 
\[[T]_{\beta}=[I]_{\alpha}^{\beta}[T]_{\alpha}[I]_{\beta}^{\alpha}=Q^{-1}AQ\]
and 
\[[U]_{\beta}=[I]_{\alpha}^{\beta}[U]_{\alpha}[I]_{\beta}^{\alpha}=Q^{-1}BQ.\]
\end{enumerate}
\item \begin{enumerate}
\item Let $\beta $ be the basis makes $T$ and $U$ simultaneously diagonalizable. We know that each pair of diagonal matrices commute. So we have 
\[[T]_{\beta}[U]_{\beta}=[U]_{\beta}[T]_{\beta}.\]
And this means $T$ and $U$ commute.
\item Let $Q$ be the invertible matrix who makes $A$ and $B$ simultaneously diagonalizable. Thus we have 
\[(Q^{-1}AQ)(Q^{-1}BQ)=(Q^{-1}BQ)(Q^{-1}AQ).\]
And this means that $A$ and $B$ commute since $Q$ is invertible.
\end{enumerate}
\item They have the same eigenvectors by Exercise 5.1.15.
\item Since we can check that
\[W_1\oplus W_2 \oplus W_3=(W_1\oplus W_2)\oplus W_3,\]
if $V$ is direct sum of $W_1,W_2,\ldots W_k$, the dimension of $V$ would be the sum of the dimension of each $W_i$ by using Exercise 1.6.29(a) inductively. 

Conversely, we first prove that if we have 
\[\sum_{i=1}^k{W_i}=V,\]
then we must have 
\[\dim(V)\leq \sum_{i=1}^k{\dim(W_i)}.\]
We induct on $k$. For $k=2$, we may use the formula in Exercise 1.6.29(a). Suppose it holds for $k=m$. We know that 
\[(\sum_{i=1}^{m-1}{W_i})+W_m=\sum_{i=1}^m{W_i}\]
and 
\[\dim(\sum_{i=1}^m{W_i})\leq \dim(\sum_{i=1}^{m-1}{W_i})+\dim(W_m)\leq \sum_{i=1}^m{\dim(W_i)}\]
by induction hypothesis and the case for $k=2$.

To prove the original statement, suppoe, by contradiction, that 
\[W_1\cap \sum{i=2}^k{W_i}\]
has nonzero element. By the formula in Exercise 1.6.29(a) we know that 
\[\dim(\sum{i=2}^k{W_i})>\dim(V)-\dim(W_1)=\sum_{i=2}^k{\dim(W_i)}.\]
This is impossible, so we get the desired result.
\item Because $\beta $ is a basis, we have 
\[\sum_{i=1}^k{\sp(\beta_i)}=V.\]
Second, since the dimension of $\sp(\beta_i)$ is the number of element of $\beta_i$, we know that the equality in the previous exercise about dimension holds. So $V$ is the direct sum of them.
\item By the definition of the left hand side, it is the sum of each eigenspaces. Let $W=\sum_{i=2}^k{E_{\lambda_i}}$. If there is some nonzero vector $v_1$ in $E_{\lambda_1}\cap W$. We may write 
\[v_1+c_2v_2+c_3v_3+\cdots +c_kv_k=0\]
for some scalar $c_i$ and some eigenvectors $v_i\in E_{\lambda_i}$. Now we know that 
\[0=T(0)=\lambda_1v_1+c_2\lambda_2v_2+c_3\lambda_3v_3+\cdots +c_k\lambda_kv_k=0.\]
After substracting this equality by $\lambda_1$ times the previous equality, we get 
\[c_2(\lambda_2-\lambda_1)v_2+\cdots +c_k(\lambda_k-\lambda_1)v_k=0.\]
This is impossible since $\lambda_i-\lambda_1$ is nonzero for all $i$ and $c_i$ cannot be all zero. Similarly we know that $E_{\lambda_i}$ has no common element other than zero with the summation of other eigenspaces. So the left hand side is the direct sum of each eigenspaces.
\item It's enough to check whether $K_1$ has common nonzero element with the summation of others or not. Thus we can do similarly for the other cases. Let $V_1$ be the summation of $K_2,K_3,\ldots ,K_p$. Now, if there is some nonzero vector $x\in K_1(V_1\cap W_2)$, we may assume that $x=u+v$ with $u\in V_1$ and $v\in W_2$. Since $W_1$ is the direct sum of all $K_i$'s, we know $K_1\cap V_1=\{0\}$. So $x-u=v\neq 0$ is an element in both $W_1$ and $W_2$, a contradiction. Thus we've completed the proof.
\end{enumerate}
